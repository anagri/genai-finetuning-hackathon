{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a15e6a18",
   "metadata": {},
   "source": [
    "# Interacting with APIs\n",
    "\n",
    "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/extras/use_cases/apis.ipynb)\n",
    "\n",
    "## Use case \n",
    "\n",
    "Suppose you want an LLM to interact with external APIs.\n",
    "\n",
    "This can be very useful for retrieving context for the LLM to utilize.\n",
    "\n",
    "And, more generally, it allows us to interact with APIs using natural langugage! \n",
    " \n",
    "\n",
    "## Overview\n",
    "\n",
    "There are two primary ways to interface LLMs with external APIs:\n",
    " \n",
    "* `Functions`: For example, [OpenAI functions](https://platform.openai.com/docs/guides/gpt/function-calling) is one popular means of doing this.\n",
    "* `LLM-generated interface`: Use an LLM with access to API documentation to create an interface.\n",
    "\n",
    "![Image description](/img/api_use_case.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbd82f0",
   "metadata": {},
   "source": [
    "## Quickstart \n",
    "\n",
    "Many APIs are already compatible with OpenAI function calling.\n",
    "\n",
    "For example, [Klarna](https://www.klarna.com/international/press/klarna-brings-smoooth-shopping-to-chatgpt/) has a YAML file that describes its API and allows OpenAI to interact with it:\n",
    "\n",
    "```\n",
    "https://www.klarna.com/us/shopping/public/openai/v0/api-docs/\n",
    "```\n",
    "\n",
    "Other options include:\n",
    "\n",
    "* [Speak](https://api.speak.com/openapi.yaml) for translation\n",
    "* [XKCD](https://gist.githubusercontent.com/roaldnefs/053e505b2b7a807290908fe9aa3e1f00/raw/0a212622ebfef501163f91e23803552411ed00e4/openapi.yaml) for comics\n",
    "\n",
    "We can supply the specification to `get_openapi_chain` directly in order to query the API with OpenAI functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a218fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain openai \n",
    "\n",
    "# Set env var OPENAI_API_KEY or load from a .env file:\n",
    "# import dotenv\n",
    "# dotenv.load_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b780e3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from langchain.chains.openai_functions.openapi import get_openapi_chain\n",
    "chain = get_openapi_chain(\"https://www.klarna.com/us/shopping/public/openai/v0/api-docs/\")\n",
    "chain(\"What are some options for a men's large blue button down shirt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9162c91c",
   "metadata": {},
   "source": [
    "## Functions \n",
    "\n",
    "We can unpack what is hapening when we use the functions to calls external APIs.\n",
    "\n",
    "Let's look at the [LangSmith trace](https://smith.langchain.com/public/76a58b85-193f-4eb7-ba40-747f0d5dd56e/r):\n",
    "\n",
    "* See [here](https://github.com/langchain-ai/langchain/blob/7fc07ba5df99b9fa8bef837b0fafa220bc5c932c/libs/langchain/langchain/chains/openai_functions/openapi.py#L279C9-L279C19) that we call the OpenAI LLM with the provided API spec:\n",
    "\n",
    "```\n",
    "https://www.klarna.com/us/shopping/public/openai/v0/api-docs/\n",
    "```\n",
    "\n",
    "* The prompt then tells the LLM to use the API spec wiith input question:\n",
    "\n",
    "```\n",
    "Use the provided API's to respond to this user query:\n",
    "What are some options for a men's large blue button down shirt\n",
    "```\n",
    "\n",
    "* The LLM returns the parameters for the function call `productsUsingGET`, which is [specified in the provided API spec](https://www.klarna.com/us/shopping/public/openai/v0/api-docs/):\n",
    "```\n",
    "function_call:\n",
    "  name: productsUsingGET\n",
    "  arguments: |-\n",
    "    {\n",
    "      \"params\": {\n",
    "        \"countryCode\": \"US\",\n",
    "        \"q\": \"men's large blue button down shirt\",\n",
    "        \"size\": 5,\n",
    "        \"min_price\": 0,\n",
    "        \"max_price\": 100\n",
    "      }\n",
    "    }\n",
    " ```\n",
    " \n",
    "![Image description](/img/api_function_call.png)\n",
    " \n",
    "* This `Dict` above split and the [API is called here](https://github.com/langchain-ai/langchain/blob/7fc07ba5df99b9fa8bef837b0fafa220bc5c932c/libs/langchain/langchain/chains/openai_functions/openapi.py#L215)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe49a0d",
   "metadata": {},
   "source": [
    "## API Chain \n",
    "\n",
    "We can also build our own interface to external APIs using the `APIChain` and provided API documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef0c3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import APIChain\n",
    "from langchain.chains.api import open_meteo_docs\n",
    "llm = OpenAI(temperature=0)\n",
    "chain = APIChain.from_llm_and_api_docs(llm, open_meteo_docs.OPEN_METEO_DOCS, verbose=True)\n",
    "chain.run('What is the weather like right now in Munich, Germany in degrees Fahrenheit?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b179318",
   "metadata": {},
   "source": [
    "Note that we supply information about the API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e03cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_meteo_docs.OPEN_METEO_DOCS[0:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fab7930",
   "metadata": {},
   "source": [
    "Under the hood, we do two things:\n",
    "    \n",
    "* `api_request_chain`: Generate an API URL based on the input question and the api_docs\n",
    "* `api_answer_chain`: generate a final answer based on the API response\n",
    "\n",
    "We can look at the [LangSmith trace](https://smith.langchain.com/public/1e0d18ca-0d76-444c-97df-a939a6a815a7/r) to inspect this:\n",
    "\n",
    "* The `api_request_chain` produces the API url from our question and the API documentation:\n",
    "\n",
    "![Image description](/img/api_chain.png)\n",
    "\n",
    "* [Here](https://github.com/langchain-ai/langchain/blob/bbd22b9b761389a5e40fc45b0570e1830aabb707/libs/langchain/langchain/chains/api/base.py#L82) we make the API request with the API url.\n",
    "* The `api_answer_chain` takes the response from the API and provides us with a natural langugae response:\n",
    "\n",
    "![Image description](/img/api_chain_response.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2511f446",
   "metadata": {},
   "source": [
    "### Going deeper\n",
    "\n",
    "**Test with other APIs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1cf418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TMDB_BEARER_TOKEN'] = \"\"\n",
    "from langchain.chains.api import tmdb_docs\n",
    "headers = {\"Authorization\": f\"Bearer {os.environ['TMDB_BEARER_TOKEN']}\"}\n",
    "chain = APIChain.from_llm_and_api_docs(llm, tmdb_docs.TMDB_DOCS, headers=headers, verbose=True)\n",
    "chain.run(\"Search for 'Avatar'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd80a717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.api import podcast_docs\n",
    "from langchain.chains import APIChain\n",
    " \n",
    "listen_api_key = 'xxx' # Get api key here: https://www.listennotes.com/api/pricing/\n",
    "llm = OpenAI(temperature=0)\n",
    "headers = {\"X-ListenAPI-Key\": listen_api_key}\n",
    "chain = APIChain.from_llm_and_api_docs(llm, podcast_docs.PODCAST_DOCS, headers=headers, verbose=True)\n",
    "chain.run(\"Search for 'silicon valley bank' podcast episodes, audio length is more than 30 minutes, return only 1 results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5939be5",
   "metadata": {},
   "source": [
    "**Web requests**\n",
    "\n",
    "URL requets are such a common use-case that we have the `LLMRequestsChain`, which makes a HTTP GET request. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b158296",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMRequestsChain, LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49c33e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Between >>> and <<< are the raw search result text from google.\n",
    "Extract the answer to the question '{query}' or say \"not found\" if the information is not contained.\n",
    "Use the format\n",
    "Extracted:<answer or \"not found\">\n",
    ">>> {requests_result} <<<\n",
    "Extracted:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"query\", \"requests_result\"],\n",
    "    template=template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fd4aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMRequestsChain(llm_chain=LLMChain(llm=OpenAI(temperature=0), prompt=PROMPT))\n",
    "question = \"What are the Three (3) biggest countries, and their respective sizes?\"\n",
    "inputs = {\n",
    "    \"query\": question,\n",
    "    \"url\": \"https://www.google.com/search?q=\" + question.replace(\" \", \"+\"),\n",
    "}\n",
    "chain(inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
