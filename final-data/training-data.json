[
    {
        "instruction": "Using the python langchain library, run a RetrievalQA operation with a custom prompt.",
        "input": "",
        "output": "```python\nfrom langchain.chains import RetrievalQA\n\nqa_chain = RetrievalQA.from_chain_type(\n    llm,\n    retriever=vectorstore.as_retriever(),\n    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n)\n\nqa_chain({\"query\": question})\n```"
    },
    {
        "instruction": "Using the Langchain library, extend your memory system to keep a sliding window of the most recent `k` interactions using `ConversationBufferWindowMemory`.",
        "input": "",
        "output": "```python\nfrom langchain.memory import ConversationBufferWindowMemory\n\nmemory = ConversationBufferWindowMemory(k=1)\nmemory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"})\nmemory.save_context({\"input\": \"not much you\"}, {\"output\": \"not much\"})\nmemory.load_memory_variables({})\n```"
    },
    {
        "instruction": "Using the python langchain library, load a text document \"novel.txt\" and split it into chunks of size 5000 with an overlap of 1000. Then, create an OpenAI embeddings for these chunks and store these embeddings in a FAISS vectorstore. Finally, convert it into a retriever.",
        "input": "",
        "output": "```python\nfrom langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores import FAISS\nfrom langchain.embeddings import OpenAIEmbeddings\n\n# Load novel.txt\nloader = TextLoader('novel.txt')\ndocuments = loader.load()\n\n# Split documents into chunks\ntext_splitter = CharacterTextSplitter(chunk_size=5000, chunk_overlap=1000)\ntexts = text_splitter.split_documents(documents)\n\n# Create OpenAI embeddings and FAISS vector-store\nembeddings = OpenAIEmbeddings()\ndb = FAISS.from_documents(texts, embeddings)\n\n# Convert to retriever\nretriever = db.as_retriever()\n```"
    },
    {
        "instruction": "Using the python langchain library, initialize the LlamaCpp LLM with the provided parameters and test it with a given prompt.",
        "input": "",
        "output": "```python\n!pip install llama-cpp-python\nfrom langchain.llms import LlamaCpp\nfrom langchain.callbacks.manager import CallbackManager\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n\n# initialize CallbackManager and LlamaCpp\ncallback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\nllm = LlamaCpp(\n    model_path=\"model_path_here.ggmlv3.q4_0.bin\",\n    n_gpu_layers=1,\n    n_batch=512,\n    n_ctx=2048,\n    f16_kv=True, \n    callback_manager=callback_manager,\n    verbose=True,\n)\n\n# test with a prompt\nprompt = \"Elaborate on Applied Machine Learning\"\nllm(prompt)\n```"
    },
    {
        "instruction": "Using the langchain library, fetch a certain topic from the vector index and generate a document using the retrieved documents as context.",
        "input": "",
        "output": "```python\ndef generate_document(topic):\n    docs = search_index.similarity_search(topic, k=3)\n    inputs = [{\"context\": doc.page_content, \"topic\": topic} for doc in docs]\n    return chain.apply(inputs)\n\ngenerate_document(\"socket programming\")\n```"
    },
    {
        "instruction": "Store the document into a vector using the Chroma method from the langchain library.",
        "input": "",
        "output": "```python\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.vectorstores import Chroma\n\nvectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())\n```"
    },
    {
        "instruction": "Using the python langchain library, create a SQL query from a user question in natural language, \"How many employees are there in the organization?\".",
        "input": "",
        "output": "```python\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.chains import create_sql_query_chain\nfrom langchain.utilities import SQLDatabase\n\n# Initialize the database \ndb = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")\n\n# Create query chain\nchain = create_sql_query_chain(ChatOpenAI(temperature=0), db)\n\n# User question \nquestion = \"How many employees are there in the organization?\"\n\n# Building the SQL query based on a user question\nresponse = chain.invoke({\"question\": question})\nsql_query = response\n```"
    },
    {
        "instruction": "Using the python langchain library, run a similarity search on a given question using the previously initialized `Chroma` instance.",
        "input": "",
        "output": "```python\nquestion = \"What are Python decorators and how are they used?\"\ndocs = vectorstore.similarity_search(question)\n```"
    },
    {
        "instruction": "Combine chat with document retrieval from the LangChain library to chat with specific information that the model was not trained on.",
        "input": "",
        "output": "```python\n!pip install tiktoken chromadb\nfrom langchain.document_loaders import WebBaseLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nloader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\ndata = loader.load()\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\nall_splits = text_splitter.split_documents(data)\n```"
    },
    {
        "instruction": "Using the python langchain library, query a SQLite database named `Chinook.db` and get the total number of employees in the database.",
        "input": "",
        "output": "```python\nfrom langchain.utilities import SQLDatabase\nfrom langchain.llms import OpenAI\nfrom langchain_experimental.sql import SQLDatabaseChain\n\n# Initializing the SQLite database\ndb = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")\n\n# Initializing the OpenAI model\nllm = OpenAI(temperature=0, verbose=True)\n\n# Creating an SQLDatabaseChain instance\ndb_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True)\n\n# Querying the database\nresult = db_chain.run(\"How many employees are there?\")\n```"
    },
    {
        "instruction": "Using the python langchain library, load a QA chain and run it with the given input documents and question.",
        "input": "",
        "output": "```python\nfrom langchain.chains.question_answering import load_qa_chain\n\ntemplate = \"\"\"Here is some context to consider for answering the question at the end. \n{context}\nQuestion: {question}\nProvide a helpful answer:\"\"\"\nQA_CHAIN_PROMPT = PromptTemplate(\n    input_variables=[\"context\", \"question\"],\n    template=template,\n)\n\nchain = load_qa_chain(llm, chain_type=\"stuff\", prompt=QA_CHAIN_PROMPT)\n\nchain({\"input_documents\": docs, \"question\": question}, return_only_outputs=True)\n```"
    },
    {
        "instruction": "Setup a memory system using ConversationBufferMemory from the langchain library for your chatbot.",
        "input": "",
        "output": "```python\nfrom langchain.memory import ConversationBufferMemory\n\nmemory = ConversationBufferMemory()\nmemory.chat_memory.add_user_message(\"hi!\")\nmemory.chat_memory.add_ai_message(\"whats up?\")\nmemory.load_memory_variables({})\n```"
    },
    {
        "instruction": "Using the python langchain library, create a tool for our previously created retriever \"search_novel\", which searches and returns chunks from the novel. Also, use an OpenAI model to create an agent executor for conversational retrieval.",
        "input": "",
        "output": "```python\nfrom langchain.agents.agent_toolkits import create_retriever_tool\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.agents.agent_toolkits import create_conversational_retrieval_agent \n\n# Create the retriever tool\ntool = create_retriever_tool(\n    retriever, \n    \"search_novel\",\n    \"Searches and returns text chunks from the novel.\"\n)\ntools = [tool]\n\n# Create an OpenAI model\nllm = ChatOpenAI(temperature = 0)\n\n# Create the agent executor\nagent_executor = create_conversational_retrieval_agent(llm, tools, verbose=True)\n```"
    },
    {
        "instruction": "Write a function to generate detailed product tutorials from a given topic and past tutorials as context, by employing the python langchain library.",
        "input": "",
        "output": "```python\ndef generate_tutorial(topic):\n    docs = search_index.similarity_search(topic, k=3)\n    inputs = [{\"context\": doc.page_content, \"topic\": topic} for doc in docs]\n    return tutorial_chain.apply(inputs)\n\ngenerate_tutorial(\"how to use language models\")\n```"
    },
    {
        "instruction": "Using the python langchain library, initialise a SelfQueryRetriever that can filter based on the metadata of the document.",
        "input": "",
        "output": "```python\n# Import required dependencies\nfrom langchain.llms import OpenAI\nfrom langchain.retrievers.self_query.base import SelfQueryRetriever\nfrom langchain.chains.query_constructor.base import AttributeInfo\n\n# Define the metadata for the retriever\nmetadata_field_info = [\n    AttributeInfo(\n        name=\"Section\",\n        description=\"Part of the document that the text comes from\",\n        type=\"string or list[string]\",\n    ),\n]\ndocument_content_description = \"Major sections of the document\"\n\n# Initialise the retriever\nllm = OpenAI(temperature=0)\nretriever = SelfQueryRetriever.from_llm(\n    llm, vectorstore, document_content_description, metadata_field_info, verbose=True\n)\n```"
    },
    {
        "instruction": "Using the langchain library, query the intelligent chatbot for various ways to implement memory to support it.",
        "input": "",
        "output": "```python\nqa(\"What are the various ways to implement memory to support it?\")\n```"
    },
    {
        "instruction": "Write a function using the python langchain library to apply inputs of context and topic to the LLM chain for text generation.",
        "input": "",
        "output": "```python\ndef generate_text(topic):\n    docs = search_index.similarity_search(topic, k=4)\n    inputs = [{\"context\": doc.page_content, \"topic\": topic} for doc in docs]\n    return chain.apply(inputs)\n```"
    },
    {
        "instruction": "Using the python langchain library, create an extraction model that extracts different parts from a user query related to a short story, specifically the conflicted character, setting, and resolution of the conflict.",
        "input": "",
        "output": "```python\nimport os\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.chains import create_extraction_chain\n\n# Set API Key\nos.environ[\"OPENAI_API_KEY\"] = 'your-openai-key'\n\n# Schema\nschema = {\n    \"properties\": {\n        \"conflicted_character\": {\"type\": \"string\"},\n        \"setting\": {\"type\": \"string\"},\n        \"resolution\": {\"type\": \"string\"},\n    },\n    \"required\": [\"conflicted_character\", \"setting\", \"resolution\"]\n}\n\n# Input \ninp = \"\"\"In a small village in England, John, a kind-hearted farmer was always in conflict with his greed. One day, he found a treasure in his field. He donated all his wealth to the village and lived happily ever after.\"\"\"\n\n# Run chain\nllm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\nchain = create_extraction_chain(schema, llm)\nchain.run(inp)\n```"
    },
    {
        "instruction": "Use the python langchain library to set up a vector index from a list of documents.",
        "input": "",
        "output": "```python\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings.openai import OpenAIEmbeddings\n\n# Assuming 'source_chunks' is a list of documents\nsearch_index = Chroma.from_documents(source_chunks, OpenAIEmbeddings())\n```"
    },
    {
        "instruction": "Using the python langchain library, interact with ListenNotes API for retrieving podcast information. The requirement is to search for 'AI in the Wild' podcast episodes, the episode audio length is more than 30 minutes and return only 3 results.",
        "input": "",
        "output": "```python\nimport os \nfrom langchain.llms import OpenAI\nfrom langchain.chains.api import podcast_docs\nfrom langchain.chains import APIChain\n\nlisten_api_key = 'your_listen_api_key' # Get api key here: https://www.listennotes.com/api/pricing/\nheaders = {\"X-ListenAPI-Key\": listen_api_key}\nchain = APIChain.from_llm_and_api_docs(OpenAI(temperature=0), podcast_docs.PODCAST_DOCS, headers=headers, verbose=True)\nresult = chain(\"Search for 'AI in the Wild' podcast episodes, audio length is more than 30 minutes, return only 3 results\")\nprint(result)\n```"
    },
    {
        "instruction": "Using the python langchain library, create a vectorstore instance with documents for similarity search.",
        "input": "",
        "output": "```python\nfrom langchain.vectorstores import MatrixStore\nfrom langchain.embeddings import HNSWeddings\n\nvectorstore = MatrixStore.from_documents(documents=all_splits, embedding=HNSWeddings())\n```"
    },
    {
        "instruction": "Using the python langchain library, set up a Language model that generates product tutorials from past documentation.",
        "input": "",
        "output": "```python\nfrom langchain.chains import LLMChain\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\n\nprompt_template = \"\"\"Use the context below to provide a detailed tutorial on the topic:\n    Context: {context}\n    Topic: {topic}\n    Tutorial:\"\"\"\n\nPROMPT = PromptTemplate(template=prompt_template, \n                        input_variables=[\"context\", \"topic\"])\n\nllm = OpenAI(temperature=0.5)\n\ntutorial_chain = LLMChain(llm=llm, prompt=PROMPT)\n```"
    },
    {
        "instruction": "Using the python langchain library, interact with the XKCD API for retrieving comic information. The requirement is to use the Klarna API documentation provided at 'https://www.klarna.com/us/shopping/public/openai/v0/api-docs/' to specify to `get_openapi_chain`.",
        "input": "",
        "output": "```python\nimport openai  \nfrom langchain.chains.openai_functions.openapi import get_openapi_chain\n# Set env var OPENAI_API_KEY or load from a .env file:\nopenai.api_key = \"your-api-key\"  \nchain = get_openapi_chain(\"https://www.klarna.com/us/shopping/public/openai/v0/api-docs/\")\nresult = chain(\"Fetch the XKCD comic number 327\")\nprint(result)\n```"
    },
    {
        "instruction": "Use the langchain library to add intelligence to your chatbot so it can retrieve accurate information while holding a conversation.",
        "input": "",
        "output": "```python\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.chains import ConversationalRetrievalChain\n\nllm = ChatOpenAI()\nretriever = vectorstore.as_retriever()\nqa = ConversationalRetrievalChain.from_llm(llm, retriever=retriever, memory=memory)\nqa(\"How do agents use Task decomposition?\")\n```"
    },
    {
        "instruction": "Use python's langchain library to index document chunks using a vector store.",
        "input": "",
        "output": "```python\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings.openai import OpenAIEmbeddings\n\n# Assuming 'source_chunks' is a list of document chunks\nsearch_index = Chroma.from_documents(source_chunks, OpenAIEmbeddings())\n```"
    },
    {
        "instruction": "Using the python langchain library, write a script that uses langchain to make API calls to a Large Language Model, given a question and context. The context is about Tom, who is a software engineer and has worked with Microsoft and Google. The question is \"What were Tom's roles at his previous jobs?\"",
        "input": "",
        "output": "```python\nfrom langchain.chains import create_citation_fuzzy_match_chain\nfrom langchain.chat_models import ChatOpenAI\n\nquestion = \"What were Tom's roles at his previous jobs?\"\ncontext = \"\"\"\nTom is a software engineer who has worked in the tech industry for over a decade.\nDuring his time at Microsoft, he was a key player in the development of their cloud infrastructure,\nand at Google, he contributed significantly to the improvement of their search algorithm.\n\"\"\"\n\nllm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0613\")\n\nchain = create_citation_fuzzy_match_chain(llm)\n\nresult = chain.run(question=question, context=context)\n\nprint(result)\n\ndef highlight(text, span):\n    return (\n        \"...\"+text[span[0] - 20 : span[0]]+\"*\"\n        + \"\\033[91m\"+text[span[0] : span[1]]+\"\\033[0m\"\n        +\"*\"+text[span[1] : span[1] + 20]+\"...\"\n    )\n\nfor fact in result.answer:\n    print(\"Statement:\", fact.fact)\n    for span in fact.get_spans(context):\n        print(\"Citation:\", highlight(context, span))\n```"
    },
    {
        "instruction": "Using the python langchain library, make use of Pydantic schema to specify the required properties and types for tagging a given text.",
        "input": "",
        "output": "```python\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.chains import create_tagging_chain_pydantic\nfrom pydantic import BaseModel, Field\n\nclass Tags(BaseModel):\n    sentiment: str = Field(..., enum=[\"happy\", \"neutral\", \"sad\"])\n    language: str = Field(..., enum=[\"english\", \"french\", \"german\", \"italian\", \"spanish\"])\n\n# LLM\nllm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0613\")\nchain = create_tagging_chain_pydantic(Tags, llm)\n\ninp = \"Je suis tr\u00e8s content de t'avoir rencontr\u00e9! Je pense que nous serons de tr\u00e8s bons amis!\"\nresult = chain.run(inp)\nprint(result)\n```"
    },
    {
        "instruction": "Using the python langchain library, parse the output of language model for a joke including setup and punchline fields by creating a custom pydantic class.",
        "input": "",
        "output": "```python\nfrom typing import Optional, List\nfrom pydantic import BaseModel, Field\nfrom langchain.prompts import PromptTemplate\nfrom langchain.llms import OpenAI\nfrom langchain.output_parsers import PydanticOutputParser\n\n# Pydantic data class\nclass JokeSchema(BaseModel):\n    setup: str = Field(description=\"setup of the joke\")\n    punchline: str = Field(description=\"punchline of the joke\")\n    \n# Parser\nparser = PydanticOutputParser(pydantic_object=JokeSchema)\n\n# Prompt\nprompt = PromptTemplate(\n    template=\"Tell a joke.\\n{format_instructions}\\n\",\n    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n)\n\n# Run model\njoke_query = \"Tell me a joke.\"\n_input = prompt.format_prompt(query=joke_query)\nmodel = OpenAI(temperature=0)\noutput = model(_input.to_string())\nparser.parse(output)\n```"
    },
    {
        "instruction": "Using the Python langchain library, summarize a list of documents using the 'map_reduce' method with a character text splitter.",
        "input": "",
        "output": "```python\n!pip install openai tiktoken chromadb\n\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.chains import ReduceDocumentsChain, MapReduceDocumentsChain\n\n# Load Documents\nloader = WebBaseLoader(\"https://your_blog_URL\")\ndocs = loader.load()\n\n# Map\nllm = ChatOpenAI(temperature=0)\ndocument_var = \"docs\"\nmap_template = f\"\"\"The following is a set of documents\n{{{document_var}}}\nBased on this list of docs, please identify the main themes\nHelpful Answer:\"\"\"\nmap_prompt = PromptTemplate.from_template(map_template)\nmap_chain = LLMChain(llm=llm, prompt=map_prompt)\n\n#Reduce\nreduce_template = f\"\"\"The following is a set of summaries:\n{{{document_var}}}\nTake these and distill it into a final, consolidated summary of the main themes.\nHelpful Answer:\"\"\"\nreduce_prompt = PromptTemplate.from_template(reduce_template)\nreduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n    \n#Combine\nmap_reduce_chain = MapReduceDocumentsChain(map_chain, reduce_chain, document_var)\n\ntext_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=1000, chunk_overlap=0)\nsplit_docs = text_splitter.split_documents(docs)\n\nsummary = map_reduce_chain.run(split_docs)\nprint(summary)\n```"
    },
    {
        "instruction": "Using the python langchain library, fetch document content from github repositories and organize them into usable data chunks.",
        "input": "",
        "output": "```python\nfrom langchain.llms import OpenAI\nfrom langchain.docstore.document import Document\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.text_splitter import CharacterTextSplitter\nimport requests\nimport subprocess\nimport tempfile\nimport pathlib\n\ndef get_github_docs(repo_owner, repo_name):\n    with tempfile.TemporaryDirectory() as d:\n        subprocess.check_call(\n            f\"git clone --depth 1 https://github.com/{repo_owner}/{repo_name}.git .\",\n            cwd=d,\n            shell=True,\n        )\n        git_sha = (\n            subprocess.check_output(\"git rev-parse HEAD\", shell=True, cwd=d)\n            .decode(\"utf-8\")\n            .strip()\n        )\n        repo_path = pathlib.Path(d)\n        markdown_files = list(repo_path.glob(\"*/*.md\")) + list(\n            repo_path.glob(\"*/*.mdx\")\n        )\n        for markdown_file in markdown_files:\n            with open(markdown_file, \"r\") as f:\n                relative_path = markdown_file.relative_to(repo_path)\n                github_url = f\"https://github.com/{repo_owner}/{repo_name}/blob/{git_sha}/{relative_path}\"\n                yield Document(page_content=f.read(), metadata={\"source\": github_url})\n\nsources = get_github_docs(\"yirenlu92\", \"deno-manual-forked\")\n\nsource_chunks = []\nsplitter = CharacterTextSplitter(separator=\" \", chunk_size=1024, chunk_overlap=0)\nfor source in sources:\n    for chunk in splitter.split_text(source.page_content):\n        source_chunks.append(Document(page_content=chunk, metadata=source.metadata))\n```"
    },
    {
        "instruction": "Using the python langchain library, split a markdown file content into sections based on the specified headers and store these sections for later retrieval.",
        "input": "",
        "output": "```python\n# Import required dependencies\nfrom langchain.document_loaders import NotionDirectoryLoader\nfrom langchain.text_splitter import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n\n# Load Markdown File\npath = \"../Markdown_Files_Path/\"\nloader = NotionDirectoryLoader(path)\ndocs = loader.load()\nmd_file = docs[0].page_content\n\n# Define headers to split on\nheaders_to_split_on = [(\"###\", \"Section\")]\nmarkdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n\n# Split the text based on headers\nmd_header_splits = markdown_splitter.split_text(md_file)\n\n# Split the text into chunks\nchunk_size = 500\nchunk_overlap = 0\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\nall_splits = text_splitter.split_documents(md_header_splits)\n```"
    },
    {
        "instruction": "Using the python langchain library, set up a simple LLM chain with a custom prompt for document generation",
        "input": "",
        "output": "```python\nfrom langchain.chains import LLMChain\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\n\nprompt_template = \"\"\"Use the context below to write a 400 word article about the topic below:\n    Context: {context}\n    Topic: {topic}\n    Article:\"\"\"\n\nPROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"topic\"])\nllm = OpenAI(temperature=0.5)\n\nchain = LLMChain(llm=llm, prompt=PROMPT)\n```"
    },
    {
        "instruction": "Using the python langchain library, control tagging results by defining possible values for each property.",
        "input": "",
        "output": "```python\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.chains import create_tagging_chain\n\n# Schema\nschema = {\n    \"properties\": {\n        \"sentiment\": {\"type\": \"string\", \"enum\": [\"happy\", \"neutral\", \"sad\"]},\n        \"language\": {\"type\": \"string\", \"enum\": [\"english\", \"french\", \"german\", \"italian\", \"spanish\"]},\n    },\n    \"required\": [\"sentiment\", \"language\"],\n}\n\n# LLM\nllm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0613\")\nchain = create_tagging_chain(schema, llm)\n\ninp = \"Estoy increiblemente contento de haberte conocido! Creo que seremos muy buenos amigos!\"\nresult = chain.run(inp)\nprint(result)\n```"
    },
    {
        "instruction": "Using the python langchain library, create a vectorstore from the markdown documents and store the indexes and metadata for faster retrieval.",
        "input": "",
        "output": "```python\n# Import the required classes\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.vectorstores import Chroma\n\n# Create the vectorstore and build the index with metadata\nvectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())\n```"
    },
    {
        "instruction": "Using the langchain library, retrieve a specific github repository, parse the markdown files and output a list of documents.",
        "input": "",
        "output": "```python\nfrom langchain.docstore.document import Document\nimport subprocess\nimport pathlib\nimport tempfile\n\ndef get_docs_from_github(repo_owner, repo_name):\n    with tempfile.TemporaryDirectory() as d:\n        subprocess.check_call(\n            f\"git clone --depth 1 https://github.com/{repo_owner}/{repo_name}.git .\",\n            cwd=d,\n            shell=True,\n        )\n        repo_path = pathlib.Path(d)\n        markdown_files = list(repo_path.glob(\"*/*.md\")) + list(repo_path.glob(\"*/*.mdx\"))\n        for markdown_file in markdown_files:\n            with open(markdown_file, \"r\") as f:\n                relative_path = markdown_file.relative_to(repo_path)\n                github_url = f\"https://github.com/{repo_owner}/{repo_name}/blob/{git_sha}/{relative_path}\"\n                yield Document(page_content=f.read(), metadata={\"source\": github_url})\n\ndocs = list(get_docs_from_github(\"userName\",\"repoName\"))\n```"
    },
    {
        "instruction": "Using the python langchain library, analyze the sentiment and language of a given text.",
        "input": "",
        "output": "```python\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.chains import create_tagging_chain\n\n# Schema\nschema = {\n    \"properties\": {\n        \"sentiment\": {\"type\": \"string\"},\n        \"language\": {\"type\": \"string\"},\n    }\n}\n\n# LLM\nllm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0613\")\nchain = create_tagging_chain(schema, llm)\n\ninp = \"I am extremely happy to have met you! I think we will be very good friends!\"\nresult = chain.run(inp)\nprint(result)\n```"
    },
    {
        "instruction": "Using the Python langchain library, download a blog post from a given URL and use the library to summarize the blog post using the 'stuff' method.",
        "input": "",
        "output": "```python\n!pip install openai tiktoken chromadb\n\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.document_loaders import WebBaseLoader\nfrom langchain.chains.summarize import load_summarize_chain\n\nloader = WebBaseLoader(\"https://your_blog_URL\")\ndocs = loader.load()\n\nllm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo-16k\")\nchain = load_summarize_chain(llm, chain_type=\"stuff\")\n\nsummary = chain.run(docs)\nprint(summary)\n```"
    },
    {
        "instruction": "Use the python langchain library to generate an informative blog post on a given topic using past articles as context.",
        "input": "",
        "output": "```python\ndef generate_blog_post(topic):\n    docs = search_index.similarity_search(topic, k=4)\n    inputs = [{\"context\": doc.page_content, \"topic\": topic} for doc in docs]\n    return chain.apply(inputs)\n\ngenerate_blog_post(\"python web scraping\")\n```"
    },
    {
        "instruction": "Using the python langchain library, download the embeddings locally and initialize `Chroma` from these documents.",
        "input": "",
        "output": "```python\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings import GPT4AllEmbeddings\n\nvectorstore = Chroma.from_documents(documents=all_splits, embedding=GPT4AllEmbeddings())\n```"
    },
    {
        "instruction": "Derive a more complex chat interface using the Langchain library, which includes a memory, ConversationSummaryMemory, and a custom prompt.",
        "input": "",
        "output": "```python\nfrom langchain.prompts import (\n    ChatPromptTemplate,\n    MessagesPlaceholder,\n    SystemMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)\nfrom langchain.chains import LLMChain\n\n# LLM\nllm = ChatOpenAI()\n\n# Prompt \nprompt = ChatPromptTemplate(\n    messages=[\n        SystemMessagePromptTemplate.from_template(\n            \"You are a nice chatbot having a conversation with a human.\"\n        ),\n        MessagesPlaceholder(variable_name=\"chat_history\"),\n        HumanMessagePromptTemplate.from_template(\"{question}\")\n    ]\n)\n\n# Memory\nmemory = ConversationSummaryMemory(llm=llm, memory_key=\"chat_history\", return_messages=True)\n\nconversation = LLMChain(llm=llm, prompt=prompt, verbose=True, memory=memory)\n\nconversation({\"question\": \"hi\"})\nconversation({\"question\": \"Translate this sentence from English to French: I love programming.\"})\n```"
    },
    {
        "instruction": "Using the python langchain library and the previously initialized GPT4All LLM, run a LLMChain operation using a PromptTemplate and retrieve documents.",
        "input": "",
        "output": "```python\nfrom langchain import PromptTemplate, LLMChain\n\nprompt = PromptTemplate.from_template(\n    \"What are the main concepts in the retrieved document: {docs}\"\n)\nllm_chain = LLMChain(llm=llm, prompt=prompt)\n\nquestion = \"Discuss Deep Learning architectures\"\ndocs = vectorstore.similarity_search(question)\nresult = llm_chain(docs)\n\nresult[\"text\"]\n```"
    },
    {
        "instruction": "Modify the previous chat interface from langchain library to remember past user inputs and model outputs by wrapping the chat model in a ConversationChain.",
        "input": "",
        "output": "```python\nfrom langchain.chains import ConversationChain\n\nconversation = ConversationChain(llm=chat)\nconversation.run(\"Translate this sentence from English to French: I love programming.\")\n```"
    },
    {
        "instruction": "Using the python langchain library, initialize the GPT4All LLM with the provided model path.",
        "input": "",
        "output": "```python\nfrom langchain.llms import GPT4All\n\nllm = GPT4All(\n    model=\"/model_path_here/nous-hermes-13b.ggmlv3.q4_0.bin\",\n    max_tokens=2048,\n)\n```"
    },
    {
        "instruction": "Using the python langchain library, create a chat interface using a plain chat model that will respond to a user message.",
        "input": "",
        "output": "```python\nfrom langchain.schema import (\n    AIMessage,\n    HumanMessage,\n    SystemMessage\n)\nfrom langchain.chat_models import ChatOpenAI\n\nchat = ChatOpenAI()\nchat([HumanMessage(content=\"Translate this sentence from English to French: I love programming.\")])\n```"
    },
    {
        "instruction": "Using the python langchain library, write a Python code to do a retrieval over two information sources, an SQL database, and a Vectorstore, concerning employee data and random text information respectively. You also need to parallelize this retrieval followed by a ChatOpenAI model invocation using the retrieved info.",
        "input": "",
        "output": "```python\n# Importing necessary classes\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.utilities import SQLDatabase\nfrom langchain.chains import create_sql_query_chain\nfrom langchain.indexes import VectorstoreIndexCreator\nfrom langchain.schema.document import Document\nfrom langchain.prompts import ChatPromptTemplate\n\n# Setup SQL query\ndb = SQLDatabase.from_uri(\"sqlite:///../../../../../notebooks/Chinook.db\")\nquery_chain = create_sql_query_chain(ChatOpenAI(temperature=0), db)\n\n# Setup vectorstore\nindex_creator = VectorstoreIndexCreator()\nindex = index_creator.from_documents([Document(page_content=\"Foo\")])\nretriever = index.vectorstore.as_retriever()\n\n# Combine the sources\nsystem_message = \"\"\"Use the information from the below two sources to answer any questions.\n\nSource 1: a SQL database about employee data\n<source1>\n{source1}\n</source1>\n\nSource 2: a text database of random information\n<source2>\n{source2}\n</source2>\n\"\"\"\n\nprompt = ChatPromptTemplate.from_messages([(\"system\", system_message), (\"human\", \"{question}\")])\n\n# Link the chains and invoke the model\nfull_chain = {\n    \"source1\": {\"question\": lambda x: x[\"question\"]} | query_chain | db.run,\n    \"source2\": (lambda x: x['question']) | retriever,\n    \"question\": lambda x: x['question'],\n} | prompt | ChatOpenAI()\n\nresponse = full_chain.invoke({\"question\": \"How many Employees are there\"})\nprint(response)\n```"
    },
    {
        "instruction": "Using the python langchain library, install the packages needed for local embeddings and vector storage, and then load and split an example document using the WebBaseLoader.",
        "input": "",
        "output": "```python\n!pip install gpt4all\n!pip install chromadb\n\nfrom langchain.document_loaders import WebBaseLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nloader = WebBaseLoader(\"https://wikipedia.org/wiki/Python_(programming_language)\")\ndata = loader.load()\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\nall_splits = text_splitter.split_documents(data)\n```"
    },
    {
        "instruction": "Extend your memory system from the LangChain library to create a summary of the conversation over time using ConversationSummaryMemory.",
        "input": "",
        "output": "```python\nfrom langchain.llms import OpenAI\nfrom langchain.memory import ConversationSummaryMemory\n\nllm = OpenAI(temperature=0)\nmemory = ConversationSummaryMemory(llm=llm)\nmemory.save_context({\"input\": \"hi\"},{\"output\": \"whats up\"})\nmemory.save_context({\"input\": \"im working on better docs for chatbots\"},{\"output\": \"oh, that sounds like a lot of work\"})\nmemory.save_context({\"input\": \"yes, but it's worth the effort\"},{\"output\": \"agreed, good docs are important!\"})\nmemory.load_memory_variables({})\n```"
    },
    {
        "instruction": "Using the python langchain library, interact with a SQL Database using sql agents created with the 'ChatOpenAI' model and get description of specific 'playlisttrack' table.",
        "input": "",
        "output": "```python\nfrom langchain.agents import create_sql_agent\nfrom langchain.agents.agent_toolkits import SQLDatabaseToolkit\nfrom langchain.llms import OpenAI\nfrom langchain.utilities import SQLDatabase\n\n# Initialize the database \ndb = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")\n\n# Create sql agent\nagent_executor = create_sql_agent(\n    llm=OpenAI(temperature=0),\n    toolkit=SQLDatabaseToolkit(db=db, llm=OpenAI(temperature=0)),\n    verbose=True,\n)\n\n# Obtaining description of the 'playlisttrack' table\nresponse = agent_executor.run(\"Describe the playlisttrack table\")\n```"
    }
]