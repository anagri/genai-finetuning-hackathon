{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import builtins\n",
    "from getpass import getpass\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_prompt = \"\"\"Think step-by-step, and do the following tasks referring the the use case document -\n",
    "1. Analyze the use case and understand the workings of langchain library\n",
    "2. The prompt format is below and starts and ends with three dash line - \"---\"\n",
    "3. The prompt format has 3 sections Preface, Instruction and Response\n",
    "4. The Preface is always - \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "5. In the Instructions section, detail the task you are trying to accomplish using langchain\n",
    "6. In the response section, write a valid python code that would accomplish the task in Instructions\n",
    "7. The prompt samples ends with a line containing a three dash line - \"---\"\n",
    "8. Follow the prompt format religiously\n",
    "9. Ensure the Instruction is complex enough to warrant a response that is at least 3 lines long python code\n",
    "10. Ensure the response is valid python code\n",
    "11. Ensure the response is moderately complex code\n",
    "12. Ensure all the necessary classes are imported in the response\n",
    "13. Assume langchain library is already installed. Provide instructions to install any other library dependencies.\n",
    "14. Combine multiple information from the use case document to create a single prompt\n",
    "15. Generate sample prompts by replacing the entitities in the use case with similar entities from knowledge base\n",
    "16. Generate at least 10 sample prompts, if possible more\n",
    "\n",
    "prompt format:\n",
    "---\n",
    "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Using the python langchain library, {instruction}\n",
    "\n",
    "### Response:\n",
    "{response}\n",
    "\n",
    "---\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated 2 prompts\n",
      "generated 1 prompts\n"
     ]
    }
   ],
   "source": [
    "files = [\"response-data/multiple_retrieval.md\", \"response-data/qa_citations.md\"]\n",
    "for file in files:\n",
    "    filename = os.path.basename(file)\n",
    "    name = os.path.splitext(filename)[0]\n",
    "    response = builtins.open(f\"{base_dir}/{file}\", 'r').read()\n",
    "    examples = response.split(\"---\")\n",
    "    examples = [e.strip() for e in examples]\n",
    "    examples = [e for e in examples if len(e) > 0]\n",
    "    print(f\"generated {len(examples)} prompts\")\n",
    "    for i, ex in enumerate(examples):\n",
    "        with builtins.open(f\"{base_dir}/training-data/{name}-{i}.md\", \"w\") as f:\n",
    "            f.write(ex)\n",
    "            f.write(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing use-case: use-cases-md/chatbots.md\n",
      "generated 10 prompts\n",
      "processing use-case: use-cases-md/summarization.md\n",
      "generated 3 prompts\n",
      "processing use-case: use-cases-md/document-context-aware-QA.md\n",
      "generated 3 prompts\n",
      "processing use-case: use-cases-md/extraction.md\n",
      "generated 5 prompts\n",
      "processing use-case: use-cases-md/apis.md\n",
      "generated 3 prompts\n",
      "processing use-case: use-cases-md/local_retrieval_qa.md\n",
      "generated 9 prompts\n",
      "processing use-case: use-cases-md/sql.md\n",
      "generated 4 prompts\n",
      "processing use-case: use-cases-md/tagging.md\n",
      "generated 4 prompts\n",
      "processing use-case: use-cases-md/conversational_retrieval_agents.md\n",
      "generated 3 prompts\n",
      "processing use-case: use-cases-md/vector_db_text_generation.md\n",
      "generated 20 prompts\n",
      "processing use-case: use-cases-md/multiple_retrieval.md\n",
      "generated 2 prompts\n",
      "processing use-case: use-cases-md/qa_citations.md\n",
      "generated 1 prompts\n"
     ]
    }
   ],
   "source": [
    "# read all files from use-cases-md\n",
    "import glob\n",
    "\n",
    "# files = glob.glob(\"use-cases-md/*.md\")\n",
    "files = [\"use-cases-md/multiple_retrieval.md\", \"use-cases-md/qa_citations.md\"]\n",
    "\n",
    "for file in files:\n",
    "    print(f\"processing use-case: {file}\")\n",
    "    doc = open(file, \"r\").read()\n",
    "    # split file into name and extension\n",
    "    filename = os.path.basename(file)\n",
    "    name, _ = os.path.splitext(filename)\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert programmer.\"},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Below is the usage for the python library langchain.\"\n",
    "                \"langchain is used to interact with Large Language Models and accomplish a variety of tasks. \"\n",
    "                \"The code below covers how to use langchain library to make API calls\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": doc},\n",
    "            {\"role\": \"user\", \"content\": generation_prompt},\n",
    "        ],\n",
    "    )\n",
    "    response = completion.choices[0].message[\"content\"]\n",
    "    with builtins.open(f\"{base_dir}/response-data/{name}.md\", \"w\") as f:\n",
    "        f.write(response)\n",
    "    # split by --- or '''\n",
    "    examples = response.split(\"---\")\n",
    "    examples = [e.strip() for e in examples]\n",
    "    examples = [e for e in examples if len(e) > 0]\n",
    "    print(f\"generated {len(examples)} prompts\")\n",
    "    for i, ex in enumerate(examples):\n",
    "        with builtins.open(f\"{base_dir}/training-data/{name}-{i}.md\", \"w\") as f:\n",
    "            f.write(ex)\n",
    "            f.write(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed file training-data/vector_db_text_generation-18.md\n",
      "removed file training-data/vector_db_text_generation-8.md\n",
      "removed file training-data/vector_db_text_generation-12.md\n",
      "removed file training-data/tagging-3.md\n",
      "removed file training-data/vector_db_text_generation-16.md\n",
      "removed file training-data/extraction-0.md\n",
      "removed file training-data/vector_db_text_generation-2.md\n",
      "removed file training-data/extraction-4.md\n",
      "removed file training-data/vector_db_text_generation-6.md\n",
      "removed file training-data/vector_db_text_generation-14.md\n",
      "removed file training-data/vector_db_text_generation-10.md\n",
      "removed file training-data/vector_db_text_generation-4.md\n",
      "removed file training-data/extraction-2.md\n",
      "removed file training-data/vector_db_text_generation-0.md\n",
      "removed file training-data/apis-2.md\n",
      "removed file training-data/sql-3.md\n",
      "removed file training-data/conversational_retrieval_agents-2.md\n",
      "removed file training-data/multiple_retrieval-1.md\n"
     ]
    }
   ],
   "source": [
    "# remove incomplete prompt files\n",
    "files = glob.glob(\"training-data/*.md\")\n",
    "for file in files:\n",
    "    # find all the lines below in file, if any don't exist, delete the file\n",
    "    with builtins.open(file, \"r\") as f:\n",
    "        # find line \"### Instruction:\", \"### Response:\", \"```python\"\n",
    "        lines = f.readlines()\n",
    "        if \"### Instruction:\\n\" not in lines or \"### Response:\\n\" not in lines or \"```python\\n\" not in lines:\n",
    "            os.remove(file)\n",
    "            print(f\"removed file {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import builtins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file training-data/*.md, find the line \"### Instruction:\"\n",
    "# copy all the content below it to variable instruction till \"### Response:\"\n",
    "# copy all the content below \"### Response:\" to variable response till end of file\n",
    "# remove the \"---\\n\" from response\n",
    "# store it as a tuple (instruction, response) in a list\n",
    "files = glob.glob(\"training-data/*.md\")\n",
    "final_data = []\n",
    "for file in files:\n",
    "    with builtins.open(f\"{base_dir}/{file}\", \"r\") as f:\n",
    "        # find line \"### Instruction:\", \"### Response:\", \"```python\"\n",
    "        lines = f.readlines()\n",
    "        current_section = \"\"\n",
    "        instruction = \"\"\n",
    "        response = \"\"\n",
    "        for line in lines:\n",
    "            if \"### Response:\" in line:\n",
    "                current_section = \"python\"\n",
    "                continue\n",
    "            if \"```python\" in line or current_section == \"python\" and not line == \"---\\n\":\n",
    "                response = response + line\n",
    "                continue\n",
    "            if \"### Instruction:\" in line:\n",
    "                current_section = \"instruction\"\n",
    "                continue\n",
    "            if current_section == \"instruction\":\n",
    "                instruction = instruction + line\n",
    "    final_data.append({\"instruction\": instruction.strip(), \"input\": \"\", \"output\": response.strip()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(final_data, open(f\"{base_dir}/final-data/training-data.json\", \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "j = json.load(open(f\"{base_dir}/final-data/training-data.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fine310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
